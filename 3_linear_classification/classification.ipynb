{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e314f71f",
      "metadata": {
        "id": "e314f71f"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b651465",
      "metadata": {
        "id": "9b651465"
      },
      "outputs": [],
      "source": [
        "!pip install d2l==1.0.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "651fef7a",
      "metadata": {
        "origin_pos": 1,
        "id": "651fef7a"
      },
      "source": [
        "# The Base Classification Model\n",
        ":label:`sec_classification`\n",
        "\n",
        "You may have noticed that the implementations from scratch and the concise implementation using framework functionality were quite similar in the case of regression. The same is true for classification. Since many models in this book deal with classification, it is worth adding functionalities to support this setting specifically. This section provides a base class for classification models to simplify future code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1dd1359",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:28.524732Z",
          "iopub.status.busy": "2023-08-18T20:14:28.524009Z",
          "iopub.status.idle": "2023-08-18T20:14:31.450273Z",
          "shell.execute_reply": "2023-08-18T20:14:31.448972Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "e1dd1359"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70bc0d4e",
      "metadata": {
        "origin_pos": 6,
        "id": "70bc0d4e"
      },
      "source": [
        "## The `Classifier` Class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "641cbe3d",
      "metadata": {
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "641cbe3d"
      },
      "source": [
        "We define the `Classifier` class below. In the `validation_step` we report both the loss value and the classification accuracy on a validation batch. We draw an update for every `num_val_batches` batches. This has the benefit of generating the averaged loss and accuracy on the whole validation data. These average numbers are not exactly correct if the final batch contains fewer examples, but we ignore this minor difference to keep the code simple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bb74037",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:31.455489Z",
          "iopub.status.busy": "2023-08-18T20:14:31.454488Z",
          "iopub.status.idle": "2023-08-18T20:14:31.461017Z",
          "shell.execute_reply": "2023-08-18T20:14:31.460154Z"
        },
        "origin_pos": 9,
        "tab": [
          "pytorch"
        ],
        "id": "1bb74037"
      },
      "outputs": [],
      "source": [
        "class Classifier(d2l.Module):  #@save\n",
        "    \"\"\"The base class of classification models.\"\"\"\n",
        "    def validation_step(self, batch):\n",
        "        Y_hat = self(*batch[:-1])\n",
        "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
        "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5244291",
      "metadata": {
        "origin_pos": 11,
        "id": "c5244291"
      },
      "source": [
        "By default we use a stochastic gradient descent optimizer, operating on minibatches, just as we did in the context of linear regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1c1bb6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:31.464177Z",
          "iopub.status.busy": "2023-08-18T20:14:31.463903Z",
          "iopub.status.idle": "2023-08-18T20:14:31.468562Z",
          "shell.execute_reply": "2023-08-18T20:14:31.467672Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "3d1c1bb6"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(d2l.Module)  #@save\n",
        "def configure_optimizers(self):\n",
        "    return torch.optim.SGD(self.parameters(), lr=self.lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28232a31",
      "metadata": {
        "origin_pos": 16,
        "id": "28232a31"
      },
      "source": [
        "## Accuracy\n",
        "\n",
        "Given the predicted probability distribution `y_hat`,\n",
        "we typically choose the class with the highest predicted probability\n",
        "whenever we must output a hard prediction.\n",
        "Indeed, many applications require that we make a choice.\n",
        "For instance, Gmail must categorize an email into \"Primary\", \"Social\", \"Updates\", \"Forums\", or \"Spam\".\n",
        "It might estimate probabilities internally,\n",
        "but at the end of the day it has to choose one among the classes.\n",
        "\n",
        "When predictions are consistent with the label class `y`, they are correct.\n",
        "The classification accuracy is the fraction of all predictions that are correct.\n",
        "Although it can be difficult to optimize accuracy directly (it is not differentiable),\n",
        "it is often the performance measure that we care about the most. It is often *the*\n",
        "relevant quantity in benchmarks. As such, we will nearly always report it when training classifiers.\n",
        "\n",
        "Accuracy is computed as follows.\n",
        "First, if `y_hat` is a matrix,\n",
        "we assume that the second dimension stores prediction scores for each class.\n",
        "We use `argmax` to obtain the predicted class by the index for the largest entry in each row.\n",
        "Then we [**compare the predicted class with the ground truth `y` elementwise.**]\n",
        "Since the equality operator `==` is sensitive to data types,\n",
        "we convert `y_hat`'s data type to match that of `y`.\n",
        "The result is a tensor containing entries of 0 (false) and 1 (true).\n",
        "Taking the sum yields the number of correct predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b132abd8",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:31.471739Z",
          "iopub.status.busy": "2023-08-18T20:14:31.471463Z",
          "iopub.status.idle": "2023-08-18T20:14:31.477124Z",
          "shell.execute_reply": "2023-08-18T20:14:31.476264Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "b132abd8"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(Classifier)  #@save\n",
        "def accuracy(self, Y_hat, Y, averaged=True):\n",
        "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n",
        "    preds = Y_hat.argmax(axis=1).type(Y.dtype)\n",
        "    compare = (preds == Y.reshape(-1)).type(torch.float32)\n",
        "    return compare.mean() if averaged else compare"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47696b41",
      "metadata": {
        "origin_pos": 20,
        "id": "47696b41"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Classification is a sufficiently common problem that it warrants its own convenience functions. Of central importance in classification is the *accuracy* of the classifier. Note that while we often care primarily about accuracy, we train classifiers to optimize a variety of other objectives for statistical and computational reasons. However, regardless of which loss function was minimized during training, it is useful to have a convenience method for assessing the accuracy of our classifier empirically.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Denote by $L_\\textrm{v}$ the validation loss, and let $L_\\textrm{v}^\\textrm{q}$ be its quick and dirty estimate computed by the loss function averaging in this section. Lastly, denote by $l_\\textrm{v}^\\textrm{b}$ the loss on the last minibatch. Express $L_\\textrm{v}$ in terms of $L_\\textrm{v}^\\textrm{q}$, $l_\\textrm{v}^\\textrm{b}$, and the sample and minibatch sizes.\n",
        "1. Show that the quick and dirty estimate $L_\\textrm{v}^\\textrm{q}$ is unbiased. That is, show that $E[L_\\textrm{v}] = E[L_\\textrm{v}^\\textrm{q}]$. Why would you still want to use $L_\\textrm{v}$ instead?\n",
        "1. Given a multiclass classification loss, denoting by $l(y,y')$ the penalty of estimating $y'$ when we see $y$ and given a probabilty $p(y \\mid x)$, formulate the rule for an optimal selection of $y'$. Hint: express the expected loss, using $l$ and $p(y \\mid x)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "283a962e"
      },
      "source": [
        "1. Denote by $L_\\textrm{v}$ the validation loss, and let $L_\\textrm{v}^\\textrm{q}$ be its quick and dirty estimate computed by the loss function averaging in this section. Lastly, denote by $l_\\textrm{v}^\\textrm{b}$ the loss on the last minibatch. Express $L_\\textrm{v}$ in terms of $L_\\textrm{v}^\\textrm{q}$, $l_\\textrm{v}^\\textrm{b}$, and the sample and minibatch sizes.\n",
        "\n",
        "Let $N$ be the total number of samples in the validation set.\n",
        "Let $B$ be the minibatch size.\n",
        "Let $M$ be the number of minibatches, so $M = \\lfloor N/B \\rfloor$.\n",
        "Let $R$ be the size of the last minibatch, so $R = N \\pmod B$. If $R=0$, then $R=B$.\n",
        "Let $L_i$ be the loss on the $i$-th minibatch.\n",
        "\n",
        "The validation loss $L_\\textrm{v}$ is the average loss over all samples:\n",
        "$L_\\textrm{v} = \\frac{1}{N} \\sum_{i=1}^M B \\cdot L_i + \\frac{1}{N} R \\cdot l_\\textrm{v}^\\textrm{b}$\n",
        "$L_\\textrm{v} = \\frac{1}{N} \\left( \\sum_{i=1}^{M-1} B \\cdot L_i + R \\cdot l_\\textrm{v}^\\textrm{b} \\right)$\n",
        "\n",
        "The quick and dirty estimate $L_\\textrm{v}^\\textrm{q}$ is the average of the losses of the first $M$ minibatches (assuming the last minibatch is included in the average):\n",
        "$L_\\textrm{v}^\\textrm{q} = \\frac{1}{M} \\sum_{i=1}^M L_i$\n",
        "\n",
        "To express $L_\\textrm{v}$ in terms of $L_\\textrm{v}^\\textrm{q}$ and $l_\\textrm{v}^\\textrm{b}$:\n",
        "We know that $\\sum_{i=1}^M L_i = M \\cdot L_\\textrm{v}^\\textrm{q}$.\n",
        "So, $L_\\textrm{v} = \\frac{1}{N} \\left( \\sum_{i=1}^{M-1} B \\cdot L_i + R \\cdot l_\\textrm{v}^\\textrm{b} \\right)$. This still includes $L_i$ for $i<M$.\n",
        "\n",
        "Let's consider the sum of losses over the first $M-1$ minibatches:\n",
        "$\\sum_{i=1}^{M-1} B \\cdot L_i = \\sum_{i=1}^{M} B \\cdot L_i - B \\cdot L_M$\n",
        "Assuming $L_M = l_\\textrm{v}^\\textrm{b}$ (the loss on the last minibatch).\n",
        "\n",
        "$L_\\textrm{v} = \\frac{1}{N} \\left( B \\sum_{i=1}^{M-1} L_i + R \\cdot l_\\textrm{v}^\\textrm{b} \\right)$\n",
        "\n",
        "We know $M \\cdot L_\\textrm{v}^\\textrm{q} = \\sum_{i=1}^M L_i = \\sum_{i=1}^{M-1} L_i + L_M$.\n",
        "So, $\\sum_{i=1}^{M-1} L_i = M \\cdot L_\\textrm{v}^\\textrm{q} - L_M = M \\cdot L_\\textrm{v}^\\textrm{q} - l_\\textrm{v}^\\textrm{b}$.\n",
        "\n",
        "Substitute this back into the expression for $L_\\textrm{v}$:\n",
        "$L_\\textrm{v} = \\frac{1}{N} \\left( B (M \\cdot L_\\textrm{v}^\\textrm{q} - l_\\textrm{v}^\\textrm{b}) + R \\cdot l_\\textrm{v}^\\textrm{b} \\right)$\n",
        "$L_\\textrm{v} = \\frac{1}{N} \\left( B M \\cdot L_\\textrm{v}^\\textrm{q} - B \\cdot l_\\textrm{v}^\\textrm{b} + R \\cdot l_\\textrm{v}^\\textrm{b} \\right)$\n",
        "$L_\\textrm{v} = \\frac{1}{N} \\left( B M \\cdot L_\\textrm{v}^\\textrm{q} + (R - B) \\cdot l_\\textrm{v}^\\textrm{b} \\right)$\n",
        "\n",
        "Since $BM + R = N$, we have $BM = N - R$.\n",
        "$L_\\textrm{v} = \\frac{1}{N} \\left( (N - R) \\cdot L_\\textrm{v}^\\textrm{q} + (R - B) \\cdot l_\\textrm{v}^\\textrm{b} \\right)$\n",
        "$L_\\textrm{v} = \\frac{N - R}{N} L_\\textrm{v}^\\textrm{q} + \\frac{R - B}{N} l_\\textrm{v}^\\textrm{b}$\n",
        "\n",
        "This expresses $L_\\textrm{v}$ in terms of $L_\\textrm{v}^\\textrm{q}$, $l_\\textrm{v}^\\textrm{b}$, and the sample and minibatch sizes ($N$, $B$, and $R$)."
      ],
      "id": "283a962e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d6a00a6"
      },
      "source": [
        "2. Show that the quick and dirty estimate $L_\\textrm{v}^\\textrm{q}$ is unbiased. That is, show that $E[L_\\textrm{v}] = E[L_\\textrm{v}^\\textrm{q}]$. Why would you still want to use $L_\\textrm{v}$ instead?\n",
        "\n",
        "**Showing $E[L_\\textrm{v}] = E[L_\\textrm{v}^\\textrm{q}]$**\n",
        "\n",
        "Let $L_i$ be the loss on the $i$-th minibatch.\n",
        "The quick and dirty estimate $L_\\textrm{v}^\\textrm{q}$ is the average of the losses of the $M$ minibatches:\n",
        "$L_\\textrm{v}^\\textrm{q} = \\frac{1}{M} \\sum_{i=1}^M L_i$\n",
        "\n",
        "The expected value of the quick and dirty estimate is:\n",
        "$E[L_\\textrm{v}^\\textrm{q}] = E\\left[\\frac{1}{M} \\sum_{i=1}^M L_i\\right]$\n",
        "By the linearity of expectation:\n",
        "$E[L_\\textrm{v}^\\textrm{q}] = \\frac{1}{M} \\sum_{i=1}^M E[L_i]$\n",
        "\n",
        "Since each minibatch is drawn randomly from the validation set, the expected loss of each minibatch is the true validation loss $L_\\textrm{v}$.\n",
        "$E[L_i] = L_\\textrm{v}$ for all $i$.\n",
        "\n",
        "Substituting this back into the expression for $E[L_\\textrm{v}^\\textrm{q}]$:\n",
        "$E[L_\\textrm{v}^\\textrm{q}] = \\frac{1}{M} \\sum_{i=1}^M L_\\textrm{v} = \\frac{1}{M} (M \\cdot L_\\textrm{v}) = L_\\textrm{v}$\n",
        "\n",
        "Therefore, $E[L_\\textrm{v}^\\textrm{q}] = L_\\textrm{v}$. Since $L_\\textrm{v}$ is a constant (the true validation loss), $E[L_\\textrm{v}] = L_\\textrm{v}$.\n",
        "Thus, $E[L_\\textrm{v}^\\textrm{q}] = E[L_\\textrm{v}]$, which shows that the quick and dirty estimate $L_\\textrm{v}^\\textrm{q}$ is an unbiased estimator of the true validation loss $L_\\textrm{v}$.\n",
        "\n",
        "**Why would you still want to use $L_\\textrm{v}$ instead?**\n",
        "\n",
        "While $L_\\textrm{v}^\\textrm{q}$ is an unbiased estimator, it is subject to higher variance compared to $L_\\textrm{v}$. $L_\\textrm{v}^\\textrm{q}$ is an average over minibatch losses, and the loss on individual minibatches can fluctuate significantly, especially with small batch sizes. This fluctuation in minibatch losses leads to a higher variance in the estimate $L_\\textrm{v}^\\textrm{q}$.\n",
        "\n",
        "The true validation loss $L_\\textrm{v}$, on the other hand, is calculated by averaging the loss over *all* samples in the validation set. This provides a more stable and reliable measure of the model's performance on the entire validation dataset. While calculating $L_\\textrm{v}$ requires iterating through all samples, it gives a more accurate picture of how well the model generalizes to the validation set compared to the quick and dirty estimate, which only considers the average of minibatch losses.\n",
        "\n",
        "In practice, especially during training, the quick and dirty estimate is often used for efficiency as it avoids a full pass over the validation set at each evaluation step. However, for final evaluation or when a precise measure of performance is needed, calculating the true validation loss $L_\\textrm{v}$ is preferable due to its lower variance and greater reliability."
      ],
      "id": "6d6a00a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb251fac"
      },
      "source": [
        "3. Given a multiclass classification loss, denoting by $l(y,y')$ the penalty of estimating $y'$ when we see $y$ and given a probability $p(y \\mid x)$, formulate the rule for an optimal selection of $y'$. Hint: express the expected loss, using $l$ and $p(y \\mid x)$.\n",
        "\n",
        "The goal is to find the optimal selection of $y'$ that minimizes the expected loss. The expected loss of estimating $y'$ for a given input $x$ is the sum of the penalties for each possible true label $y$, weighted by the probability of that true label given $x$:\n",
        "\n",
        "$E[\\text{loss} \\mid x, y'] = \\sum_{y} l(y, y') p(y \\mid x)$\n",
        "\n",
        "To find the optimal $y'$, we need to choose the $y'$ that minimizes this expected loss:\n",
        "\n",
        "Optimal $y' = \\arg\\min_{y'} \\sum_{y} l(y, y') p(y \\mid x)$\n",
        "\n",
        "This rule states that for a given input $x$, the optimal predicted class $y'$ is the one that minimizes the sum of the penalties for all possible true classes $y$, where each penalty is weighted by the probability of that true class given $x$.\n",
        "\n",
        "A common example of a loss function is the 0-1 loss, where $l(y, y') = 0$ if $y = y'$ and $l(y, y') = 1$ if $y \\neq y'$. In this case, the expected loss becomes:\n",
        "\n",
        "$E[\\text{loss} \\mid x, y'] = \\sum_{y \\neq y'} 1 \\cdot p(y \\mid x) = \\sum_{y \\neq y'} p(y \\mid x)$\n",
        "\n",
        "Since $\\sum_{y} p(y \\mid x) = 1$, we have $\\sum_{y \\neq y'} p(y \\mid x) = 1 - p(y' \\mid x)$.\n",
        "\n",
        "So, for the 0-1 loss, the optimal $y'$ is the one that minimizes $1 - p(y' \\mid x)$, which is equivalent to maximizing $p(y' \\mid x)$:\n",
        "\n",
        "Optimal $y'$ (with 0-1 loss) $= \\arg\\max_{y'} p(y' \\mid x)$\n",
        "\n",
        "This means that with the 0-1 loss, the optimal strategy is to predict the class with the highest posterior probability, which is the maximum a posteriori (MAP) decision rule."
      ],
      "id": "eb251fac"
    },
    {
      "cell_type": "markdown",
      "id": "398f847c",
      "metadata": {
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "id": "398f847c"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/6809)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}