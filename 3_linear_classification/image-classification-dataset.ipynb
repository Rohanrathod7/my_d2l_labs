{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4fb38af9",
      "metadata": {
        "id": "4fb38af9"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "41091c46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41091c46",
        "outputId": "4bb4cdd5-594d-4877-ba9f-f5983c09ea11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting d2l==1.0.3\n",
            "  Downloading d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
            "Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: d2l\n",
            "Successfully installed d2l-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.3 --no-deps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e57838cd",
      "metadata": {
        "origin_pos": 1,
        "id": "e57838cd"
      },
      "source": [
        "# The Image Classification Dataset\n",
        ":label:`sec_fashion_mnist`\n",
        "\n",
        "(~~The MNIST dataset is one of the widely used dataset for image classification, while it is too simple as a benchmark dataset. We will use the similar, but more complex Fashion-MNIST dataset ~~)\n",
        "\n",
        "One widely used dataset for image classification is the  [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) :cite:`LeCun.Bottou.Bengio.ea.1998` of handwritten digits. At the time of its release in the 1990s it posed a formidable challenge to most machine learning algorithms, consisting of 60,000 images of $28 \\times 28$ pixels resolution (plus a test dataset of 10,000 images). To put things into perspective, back in 1995, a Sun SPARCStation 5 with a whopping 64MB of RAM and a blistering 5 MFLOPs was considered state of the art equipment for machine learning at AT&T Bell Laboratories. Achieving high accuracy on digit recognition was a key component in automating letter sorting for the USPS in the 1990s. Deep networks such as LeNet-5 :cite:`LeCun.Jackel.Bottou.ea.1995`, support vector machines with invariances :cite:`Scholkopf.Burges.Vapnik.1996`, and tangent distance classifiers :cite:`Simard.LeCun.Denker.ea.1998` all could reach error rates below 1%.\n",
        "\n",
        "For over a decade, MNIST served as *the* point of reference for comparing machine learning algorithms.\n",
        "While it had a good run as a benchmark dataset,\n",
        "even simple models by today's standards achieve classification accuracy over 95%,\n",
        "making it unsuitable for distinguishing between strong models and weaker ones. Even more, the dataset allows for *very* high levels of accuracy, not typically seen in many classification problems. This skewed algorithmic development towards specific families of algorithms that can take advantage of clean datasets, such as active set methods and boundary-seeking active set algorithms.\n",
        "Today, MNIST serves as more of a sanity check than as a benchmark. ImageNet :cite:`Deng.Dong.Socher.ea.2009` poses a much\n",
        "more relevant challenge. Unfortunately, ImageNet is too large for many of the examples and illustrations in this book, as it would take too long to train to make the examples interactive. As a substitute we will focus our discussion in the coming sections on the qualitatively similar, but much smaller Fashion-MNIST\n",
        "dataset :cite:`Xiao.Rasul.Vollgraf.2017` which was released in 2017. It contains images of 10 categories of clothing at $28 \\times 28$ pixels resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "270279ba",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:35:53.123984Z",
          "iopub.status.busy": "2023-08-18T19:35:53.123639Z",
          "iopub.status.idle": "2023-08-18T19:35:56.952902Z",
          "shell.execute_reply": "2023-08-18T19:35:56.951810Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "270279ba"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from d2l import torch as d2l\n",
        "\n",
        "d2l.use_svg_display()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1070bad5",
      "metadata": {
        "origin_pos": 6,
        "id": "1070bad5"
      },
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "Since the Fashion-MNIST dataset is so useful, all major frameworks provide preprocessed versions of it. We can  [**download and read it into memory using built-in framework utilities.**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b2e83e15",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:35:56.959608Z",
          "iopub.status.busy": "2023-08-18T19:35:56.958972Z",
          "iopub.status.idle": "2023-08-18T19:35:56.974091Z",
          "shell.execute_reply": "2023-08-18T19:35:56.969189Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "b2e83e15"
      },
      "outputs": [],
      "source": [
        "class FashionMNIST(d2l.DataModule):  #@save\n",
        "    \"\"\"The Fashion-MNIST dataset.\"\"\"\n",
        "    def __init__(self, batch_size=64, resize=(28, 28)):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        trans = transforms.Compose([transforms.Resize(resize),\n",
        "                                    transforms.ToTensor()])\n",
        "        self.train = torchvision.datasets.FashionMNIST(\n",
        "            root=self.root, train=True, transform=trans, download=True)\n",
        "        self.val = torchvision.datasets.FashionMNIST(\n",
        "            root=self.root, train=False, transform=trans, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ac7e84e",
      "metadata": {
        "origin_pos": 10,
        "id": "2ac7e84e"
      },
      "source": [
        "Fashion-MNIST consists of images from 10 categories, each represented\n",
        "by 6000 images in the training dataset and by 1000 in the test dataset.\n",
        "A *test dataset* is used for evaluating model performance (it must not be used for training).\n",
        "Consequently the training set and the test set\n",
        "contain 60,000 and 10,000 images, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9702ba11",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:35:56.979230Z",
          "iopub.status.busy": "2023-08-18T19:35:56.978838Z",
          "iopub.status.idle": "2023-08-18T19:35:57.112651Z",
          "shell.execute_reply": "2023-08-18T19:35:57.111496Z"
        },
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9702ba11",
        "outputId": "ca8dfae9-c64a-445a-ad0e-cba170b3bd7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.0MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 301kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.03MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 6.04MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data = FashionMNIST(resize=(32, 32))\n",
        "len(data.train), len(data.val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a66cd0bb",
      "metadata": {
        "origin_pos": 13,
        "id": "a66cd0bb"
      },
      "source": [
        "The images are grayscale and upscaled to $32 \\times 32$ pixels in resolution above. This is similar to the original MNIST dataset which consisted of (binary) black and white images. Note, though, that most modern image data has three channels (red, green, blue) and that hyperspectral images can have in excess of 100 channels (the HyMap sensor has 126 channels).\n",
        "By convention we store an image as a $c \\times h \\times w$ tensor, where $c$ is the number of color channels, $h$ is the height and $w$ is the width.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b31548fa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:35:57.116730Z",
          "iopub.status.busy": "2023-08-18T19:35:57.116328Z",
          "iopub.status.idle": "2023-08-18T19:35:57.128533Z",
          "shell.execute_reply": "2023-08-18T19:35:57.127453Z"
        },
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b31548fa",
        "outputId": "f12afe92-855d-4fe6-ffb7-56910a673f98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data.train[0][0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e625e5",
      "metadata": {
        "origin_pos": 15,
        "id": "b2e625e5"
      },
      "source": [
        "[~~Two utility functions to visualize the dataset~~]\n",
        "\n",
        "The categories of Fashion-MNIST have human-understandable names.\n",
        "The following convenience method converts between numeric labels and their names.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ca95ebc5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:35:57.137128Z",
          "iopub.status.busy": "2023-08-18T19:35:57.136465Z",
          "iopub.status.idle": "2023-08-18T19:35:57.142322Z",
          "shell.execute_reply": "2023-08-18T19:35:57.141204Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "ca95ebc5"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(FashionMNIST)  #@save\n",
        "def text_labels(self, indices):\n",
        "    \"\"\"Return text labels.\"\"\"\n",
        "    labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "              'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "    return [labels[int(i)] for i in indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a87f298",
      "metadata": {
        "origin_pos": 17,
        "id": "4a87f298"
      },
      "source": [
        "## Reading a Minibatch\n",
        "\n",
        "To make our life easier when reading from the training and test sets,\n",
        "we use the built-in data iterator rather than creating one from scratch.\n",
        "Recall that at each iteration, a data iterator\n",
        "[**reads a minibatch of data with size `batch_size`.**]\n",
        "We also randomly shuffle the examples for the training data iterator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8982acc7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:35:57.146600Z",
          "iopub.status.busy": "2023-08-18T19:35:57.145989Z",
          "iopub.status.idle": "2023-08-18T19:35:57.153720Z",
          "shell.execute_reply": "2023-08-18T19:35:57.150894Z"
        },
        "origin_pos": 19,
        "tab": [
          "pytorch"
        ],
        "id": "8982acc7"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(FashionMNIST)  #@save\n",
        "def get_dataloader(self, train):\n",
        "    data = self.train if train else self.val\n",
        "    return torch.utils.data.DataLoader(data, self.batch_size, shuffle=train,\n",
        "                                       num_workers=self.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6058b32",
      "metadata": {
        "origin_pos": 21,
        "id": "f6058b32"
      },
      "source": [
        "To see how this works, let's load a minibatch of images by invoking the `train_dataloader` method. It contains 64 images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "81f8afca",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:35:57.160131Z",
          "iopub.status.busy": "2023-08-18T19:35:57.159304Z",
          "iopub.status.idle": "2023-08-18T19:35:57.397652Z",
          "shell.execute_reply": "2023-08-18T19:35:57.396242Z"
        },
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81f8afca",
        "outputId": "ab3cce9f-aedb-410b-c462-768dd17255d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 32, 32]) torch.float32 torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "X, y = next(iter(data.train_dataloader()))\n",
        "print(X.shape, X.dtype, y.shape, y.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51090d6f",
      "metadata": {
        "origin_pos": 23,
        "id": "51090d6f"
      },
      "source": [
        "Let's look at the time it takes to read the images. Even though it is a built-in loader, it is not blazingly fast. Nonetheless, this is sufficient since processing images with a deep network takes quite a bit longer. Hence it is good enough that training a network will not be I/O constrained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "47e90ba5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:35:57.402928Z",
          "iopub.status.busy": "2023-08-18T19:35:57.402306Z",
          "iopub.status.idle": "2023-08-18T19:36:02.097749Z",
          "shell.execute_reply": "2023-08-18T19:36:02.096784Z"
        },
        "origin_pos": 24,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "47e90ba5",
        "outputId": "b596b0b4-7e2a-42ea-985f-2d46d047d859"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'17.06 sec'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tic = time.time()\n",
        "for X, y in data.train_dataloader():\n",
        "    continue\n",
        "f'{time.time() - tic:.2f} sec'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2887996a",
      "metadata": {
        "origin_pos": 25,
        "id": "2887996a"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "We will often be using the Fashion-MNIST dataset. A convenience function `show_images` can be used to visualize the images and the associated labels.\n",
        "Skipping implementation details, we just show the interface below: we only need to know how to invoke `d2l.show_images` rather than how it works\n",
        "for such utility functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "06fb4e72",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:36:02.101877Z",
          "iopub.status.busy": "2023-08-18T19:36:02.101254Z",
          "iopub.status.idle": "2023-08-18T19:36:02.105863Z",
          "shell.execute_reply": "2023-08-18T19:36:02.105019Z"
        },
        "origin_pos": 26,
        "tab": [
          "pytorch"
        ],
        "id": "06fb4e72"
      },
      "outputs": [],
      "source": [
        "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n",
        "    \"\"\"Plot a list of images.\"\"\"\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e35cb97",
      "metadata": {
        "origin_pos": 27,
        "id": "2e35cb97"
      },
      "source": [
        "Let's put it to good use. In general, it is a good idea to visualize and inspect data that you are training on.\n",
        "Humans are very good at spotting oddities and because of that, visualization serves as an additional safeguard against mistakes and errors in the design of experiments. Here are [**the images and their corresponding labels**] (in text)\n",
        "for the first few examples in the training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b16ddca3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:36:02.109988Z",
          "iopub.status.busy": "2023-08-18T19:36:02.109439Z",
          "iopub.status.idle": "2023-08-18T19:36:02.819862Z",
          "shell.execute_reply": "2023-08-18T19:36:02.811142Z"
        },
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "b16ddca3",
        "outputId": "2f5806d8-7b5e-4f72-dffb-5f631c360c95"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x150 with 8 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"684pt\" height=\"100.752168pt\" viewBox=\"0 0 684 100.752168\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-10-24T22:31:44.645919</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.10.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 100.752168 \nL 684 100.752168 \nL 684 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 7.2 93.552168 \nL 78.434043 93.552168 \nL 78.434043 22.318125 \nL 7.2 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#p6e1a6634ad)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image2441973e71\" transform=\"scale(1 -1) translate(0 -71.28)\" x=\"7.2\" y=\"-22.272168\" width=\"71.28\" height=\"71.28\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 7.2 93.552168 \nL 7.2 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 78.434043 93.552168 \nL 78.434043 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 7.2 93.552168 \nL 78.434043 93.552168 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 7.2 22.318125 \nL 78.434043 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_1\">\n    <!-- ankle boot -->\n    <g transform=\"translate(11.093896 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \nL 1159 4863 \nL 1159 1991 \nL 2875 3500 \nL 3609 3500 \nL 1753 1863 \nL 3688 0 \nL 2938 0 \nL 1159 1709 \nL 1159 0 \nL 581 0 \nL 581 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-61\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(61.279297 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6b\" transform=\"translate(124.658203 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(182.568359 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(210.351562 0)\"/>\n     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(271.875 0)\"/>\n     <use xlink:href=\"#DejaVuSans-62\" transform=\"translate(303.662109 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(367.138672 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(428.320312 0)\"/>\n     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(489.501953 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 92.680851 93.552168 \nL 163.914894 93.552168 \nL 163.914894 22.318125 \nL 92.680851 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pd9b9bdaa33)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"imagef7f91c5a64\" transform=\"scale(1 -1) translate(0 -71.28)\" x=\"92.680851\" y=\"-22.272168\" width=\"71.28\" height=\"71.28\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 92.680851 93.552168 \nL 92.680851 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 163.914894 93.552168 \nL 163.914894 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 92.680851 93.552168 \nL 163.914894 93.552168 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 92.680851 22.318125 \nL 163.914894 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_2\">\n    <!-- pullover -->\n    <g transform=\"translate(103.971622 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-70\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(63.476562 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(126.855469 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(154.638672 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(182.421875 0)\"/>\n     <use xlink:href=\"#DejaVuSans-76\" transform=\"translate(243.603516 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(302.783203 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(364.306641 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 178.161702 93.552168 \nL 249.395745 93.552168 \nL 249.395745 22.318125 \nL 178.161702 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pcdc20afb5e)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image94fa8c41bb\" transform=\"scale(1 -1) translate(0 -71.28)\" x=\"178.161702\" y=\"-22.272168\" width=\"71.28\" height=\"71.28\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 178.161702 93.552168 \nL 178.161702 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 249.395745 93.552168 \nL 249.395745 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 178.161702 93.552168 \nL 249.395745 93.552168 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 178.161702 22.318125 \nL 249.395745 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_3\">\n    <!-- trouser -->\n    <g transform=\"translate(192.337161 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-74\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(78.072266 0)\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(139.253906 0)\"/>\n     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(202.632812 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(254.732422 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(316.255859 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_17\">\n    <path d=\"M 263.642553 93.552168 \nL 334.876596 93.552168 \nL 334.876596 22.318125 \nL 263.642553 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pae8be62f96)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image5797d892a7\" transform=\"scale(1 -1) translate(0 -71.28)\" x=\"263.642553\" y=\"-22.272168\" width=\"71.28\" height=\"71.28\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 263.642553 93.552168 \nL 263.642553 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 334.876596 93.552168 \nL 334.876596 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 263.642553 93.552168 \nL 334.876596 93.552168 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 263.642553 22.318125 \nL 334.876596 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_4\">\n    <!-- trouser -->\n    <g transform=\"translate(277.818012 16.318125) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-74\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(78.072266 0)\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(139.253906 0)\"/>\n     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(202.632812 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(254.732422 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(316.255859 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_5\">\n   <g id=\"patch_22\">\n    <path d=\"M 349.123404 93.552168 \nL 420.357447 93.552168 \nL 420.357447 22.318125 \nL 349.123404 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#p1d5daa017d)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"imageb67ac0f6d9\" transform=\"scale(1 -1) translate(0 -71.28)\" x=\"349.123404\" y=\"-22.272168\" width=\"71.28\" height=\"71.28\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 349.123404 93.552168 \nL 349.123404 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 420.357447 93.552168 \nL 420.357447 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path d=\"M 349.123404 93.552168 \nL 420.357447 93.552168 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path d=\"M 349.123404 22.318125 \nL 420.357447 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_5\">\n    <!-- shirt -->\n    <g transform=\"translate(371.326676 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-73\"/>\n     <use xlink:href=\"#DejaVuSans-68\" transform=\"translate(52.099609 0)\"/>\n     <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(115.478516 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(143.261719 0)\"/>\n     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(184.375 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_6\">\n   <g id=\"patch_27\">\n    <path d=\"M 434.604255 93.552168 \nL 505.838298 93.552168 \nL 505.838298 22.318125 \nL 434.604255 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#p708847b8c0)\">\n    <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAGMAAABjCAYAAACPO76VAAAILUlEQVR4nO1dW29UVRTe58yZa2c6vU2BFtoBCi0YMAjGaMQIGtSgEqMx8Ul+gI/6aPgJPumbf4BEEjXRJzEIBgnBcFGQi0gLtPTezjAznTk3n1zfOmUaBmzP7GTW9/TN6Tl7n+ma/Z219l5rH+N14wNfaQZrw3riswfzxKcO1Yj7FYv4ph+D16d//ou4WyiAv/oc8bFDceLe4BIunsDxvtMe8dQPF9G3jftYTZhr0qrgqSDG0AjW408JH4UXB4knj04QPzF0nPiY00n804WPA9dv+72dNQaZmtmdIP7OG78R/6TnF+KnK3nix7LvER85CfkSmWoBiDE0QvgyZRjgfn1HrpzDb+TD9VeJuwrXdkceEm//O3i9X6nUbddj33Z9fJH45mia+E0bx5UZrqMpI0MjiDE0QhNkitsfQZXV30e8CGdKDSfgTd11uojfrK4jnjs3H+jCWyzW7dpnXXvsw4xbIj7nbsRJIlOtCzGGRtAm6Cs/s4G4tR0y02dBgsZZoHdufjNxYwxSppRS3gpBmYu4TaVMnFP0IEcJ036Cu15dyMjQCGIMjRC+TPke45CHuZEY8YODV4hHFM6p+RHiF27B5RpxbzTUtRdHW9kIPCh2RypjImA0Suzf4/Gz1gYyMjSCGEMjiDE0QhOeGdBtI4rnxMM8NPmtzkvEbfacuFfrJp47hWv9WmPrCx4uUW3Mta2xaNz2+XOioWZXDTIyNIIYQyM0NQI3B/vxoadKNGEgCp5yM8RvV3I4/fwccdd2GuqPTxRGDVxTZVJY8LA0a9bY2ksIkJGhEcQYGqGpMnX/bUwOHt5xnngHi4KnHWR6XFvEGoZ17RYa8tyG+nMTcI96I5iM5Mu5POJ3s6xdc+1/tzIyNIIYQyM0VaYKw/BodqbGic+6bcRzFpLQcklkhMw3KE0BtKO/XARS+CtLXJt0sjhfgr7WhRhDIzR32ZXFVGUPa6K2QhBWdJPEL40jSBxQCPqeBheryEY5MbmXeGe8jNtzeDqJrGe0FMQYGqG5ubYMS16UuMsmkS6U8sSjZzOqLsxI8DNb2jVTKRx20fepwjDxqxMIJgdzyEbxDZbEJkFfa0GMoRFClylz9wjxaBbT5hED0sJX287OIFmt/ydICPdtjEhQpnwHfzX6UawZS2N1r+Ji2c8ugZc7IJeGK1PoLQsxhkYIXaYmXkG+bG/nA+Lcg+KrcKUaJCRx+94T91caRhLD1l5cHzfZ6qCNvotLCD4jFQn6WhZiDI0QukyV+xFIdVuQiikbAV13GlPl+3rvEr+zaQANXa1fnbQcM7vwFUfa4I3dKvYQN6r4TZYeIiEhEJ+uUAy6mpCRoRHEGBohdJnyLAx3z4cOjFewwnY/A4+rL75A/PKOZ4m3oTxc+e6yVT8mKZU+/G1LcgZ9lDtwehvkMhZj+VRp1u6ywHItICNDI4gxNIIYQyOE/8xIslKuGHZAs9hE4Y0lJLeZLKlsYj+eMUPf8EZXzhSxepAFsjk+RfxSFMX37T0oKdvQjmyU0T/Z7gDLn0trABkZGkGMoRHCX3bNIN1/XRKS4HhwHRcdZITsbbtDfNeef4jX38ToUXRnIUEDFjJKehOI4MudmIzMJRD9jzlMpkKAjAyNIMbQCKHLlMUi3O4oJOQh29iDL4nyvTzezP1B/Ps8Es+cUUwmKqUCEfi6FGSH59f2RHHcSUEiPV4ewLcRkfWM1oIYQyOELlNODV2mI8gOcdnvYsGGN8X3C3k+CW/qq/ePEO//EsGcUkoZGxE05tOQsDhbn4gaCOLaLcgXl0sV7kZsMjJ0ghhDI4QuU8YcPCWeuMZ3RpvzUbm0wKqYUjHsuJZ/9zZx52smLUqpqQNIXDuaOUncZrLDs1F4pgiXS3/tlzACkJGhEcQYGiF0mUrdh/15GQD3bhwmIWW2Fc4iq246NvAd8c+jhwN9zO2Hl/ZS4j7aYsu8PLgL3B+TS09kqnUhxtAIoctUZgweVMFBwlhvDFPaVRe3xbfJfuAig+TlxCTO382S25RSB7ZfJ95lQubmnPrzS1WvfiDK9wGTyqUWgxhDI4QvU6Oos16wUfy4LQnZyUSRqGCywJDPU91zcOuvfXEm0MdH2QvEeU35EtMd7jVx8Cl7Ny5vCWhZiDE0QvgrfRPzdY93RMp1jwemuk3IV8FHAPhZ99XANbYPD2rChRxNu8jhLbrw5PgcWTqCPrwYk6kV6tdXEzIyNIIYQyOELlPuOLym2SpL/Y9CvvrYa9sirOKbF17y7VYvVIO/qQTbZsJVkKwSm9vic14ZJk38lXOeeFOtCzGGRghdpvhLam/NosixbQDHA6+Ds/E6uMCm8QwJI7jjc8nH1PwS4/z6LMuh6rIgTXxb1dg8+61KgWVrQYyhEcQYGqGpG0a6F7E+cWUnKoleSNwhvuCm1OPwt50LfObPCf5CK+4OZ636Ef/FEsoAOm7gOfFIRe0aQEaGRhBjaISmylT/aUS+xw/sI35k6A7xfAyF9CaLxvlu/g/4ltnLwCNqPiHIMVbFNkinJoeId51H342+MOX/QEaGRhBjaISmylTsyijxG+PI8FjaCjnh3hDPg+UylY9NB9rlkTa/fsGDZ8azTnh2yPRCmnj6+uUGvsXqQUaGRhBjaISmypRfgTflLiGLY9JF0DbrQja4N5Rg1Y8ZthyrlFJF9no3Lk28vCDDJgqHU9i48mRye+NfYJUhI0MjiDE0QlNlyiuhDjz2ANL0bWEPcV7wyL2eJJOpM5NbAu0m2EaUw9lg8eV/qDApjLHKpardvH+JjAyNIMbQCP8Cw+KAIZgXVoMAAAAASUVORK5CYII=\" id=\"image3f28a9c899\" transform=\"scale(1 -1) translate(0 -71.28)\" x=\"434.604255\" y=\"-22.272168\" width=\"71.28\" height=\"71.28\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 434.604255 93.552168 \nL 434.604255 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 505.838298 93.552168 \nL 505.838298 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 434.604255 93.552168 \nL 505.838298 93.552168 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 434.604255 22.318125 \nL 505.838298 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_6\">\n    <!-- trouser -->\n    <g transform=\"translate(448.779714 16.318125) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-74\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(78.072266 0)\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(139.253906 0)\"/>\n     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(202.632812 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(254.732422 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(316.255859 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_7\">\n   <g id=\"patch_32\">\n    <path d=\"M 520.085106 93.552168 \nL 591.319149 93.552168 \nL 591.319149 22.318125 \nL 520.085106 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pd0fdabb320)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image65fa1fe4be\" transform=\"scale(1 -1) translate(0 -71.28)\" x=\"520.085106\" y=\"-22.272168\" width=\"71.28\" height=\"71.28\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 520.085106 93.552168 \nL 520.085106 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 591.319149 93.552168 \nL 591.319149 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 520.085106 93.552168 \nL 591.319149 93.552168 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 520.085106 22.318125 \nL 591.319149 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_7\">\n    <!-- coat -->\n    <g transform=\"translate(542.702753 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-63\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(54.980469 0)\"/>\n     <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(116.162109 0)\"/>\n     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(177.441406 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_8\">\n   <g id=\"patch_37\">\n    <path d=\"M 605.565957 93.552168 \nL 676.8 93.552168 \nL 676.8 22.318125 \nL 605.565957 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#p60bad621b2)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image518ba10955\" transform=\"scale(1 -1) translate(0 -71.28)\" x=\"605.565957\" y=\"-22.272168\" width=\"71.28\" height=\"71.28\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path d=\"M 605.565957 93.552168 \nL 605.565957 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path d=\"M 676.8 93.552168 \nL 676.8 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path d=\"M 605.565957 93.552168 \nL 676.8 93.552168 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path d=\"M 605.565957 22.318125 \nL 676.8 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_8\">\n    <!-- shirt -->\n    <g transform=\"translate(627.769229 16.318125) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-73\"/>\n     <use xlink:href=\"#DejaVuSans-68\" transform=\"translate(52.099609 0)\"/>\n     <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(115.478516 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(143.261719 0)\"/>\n     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(184.375 0)\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6e1a6634ad\">\n   <rect x=\"7.2\" y=\"22.318125\" width=\"71.234043\" height=\"71.234043\"/>\n  </clipPath>\n  <clipPath id=\"pd9b9bdaa33\">\n   <rect x=\"92.680851\" y=\"22.318125\" width=\"71.234043\" height=\"71.234043\"/>\n  </clipPath>\n  <clipPath id=\"pcdc20afb5e\">\n   <rect x=\"178.161702\" y=\"22.318125\" width=\"71.234043\" height=\"71.234043\"/>\n  </clipPath>\n  <clipPath id=\"pae8be62f96\">\n   <rect x=\"263.642553\" y=\"22.318125\" width=\"71.234043\" height=\"71.234043\"/>\n  </clipPath>\n  <clipPath id=\"p1d5daa017d\">\n   <rect x=\"349.123404\" y=\"22.318125\" width=\"71.234043\" height=\"71.234043\"/>\n  </clipPath>\n  <clipPath id=\"p708847b8c0\">\n   <rect x=\"434.604255\" y=\"22.318125\" width=\"71.234043\" height=\"71.234043\"/>\n  </clipPath>\n  <clipPath id=\"pd0fdabb320\">\n   <rect x=\"520.085106\" y=\"22.318125\" width=\"71.234043\" height=\"71.234043\"/>\n  </clipPath>\n  <clipPath id=\"p60bad621b2\">\n   <rect x=\"605.565957\" y=\"22.318125\" width=\"71.234043\" height=\"71.234043\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "@d2l.add_to_class(FashionMNIST)  #@save\n",
        "def visualize(self, batch, nrows=1, ncols=8, labels=[]):\n",
        "    X, y = batch\n",
        "    if not labels:\n",
        "        labels = self.text_labels(y)\n",
        "    d2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)\n",
        "batch = next(iter(data.val_dataloader()))\n",
        "data.visualize(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437083bc",
      "metadata": {
        "origin_pos": 29,
        "id": "437083bc"
      },
      "source": [
        "We are now ready to work with the Fashion-MNIST dataset in the sections that follow.\n",
        "\n",
        "## Summary\n",
        "\n",
        "We now have a slightly more realistic dataset to use for classification. Fashion-MNIST is an apparel classification dataset consisting of images representing 10 categories. We will use this dataset in subsequent sections and chapters to evaluate various network designs, from a simple linear model to advanced residual networks. As we commonly do with images, we read them as a tensor of shape (batch size, number of channels, height, width). For now, we only have one channel as the images are grayscale (the visualization above uses a false color palette for improved visibility).\n",
        "\n",
        "Lastly, data iterators are a key component for efficient performance. For instance, we might use GPUs for efficient image decompression, video transcoding, or other preprocessing. Whenever possible, you should rely on well-implemented data iterators that exploit high-performance computing to avoid slowing down your training loop.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Does reducing the `batch_size` (for instance, to 1) affect the reading performance?\n",
        "1. The data iterator performance is important. Do you think the current implementation is fast enough? Explore various options to improve it. Use a system profiler to find out where the bottlenecks are.\n",
        "1. Check out the framework's online API documentation. Which other datasets are available?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1."
      ],
      "metadata": {
        "id": "C6I-iDDhV-Ch"
      },
      "id": "C6I-iDDhV-Ch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "26c54b96",
        "outputId": "b21491e1-438b-40d4-a83e-3e4fb8e68651"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from d2l import torch as d2l\n",
        "\n",
        "class FashionMNIST(d2l.DataModule):  #@save\n",
        "    \"\"\"The Fashion-MNIST dataset.\"\"\"\n",
        "    def __init__(self, batch_size=64, resize=(28, 28)):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        trans = transforms.Compose([transforms.Resize(resize),\n",
        "                                    transforms.ToTensor()])\n",
        "        self.train = torchvision.datasets.FashionMNIST(\n",
        "            root=self.root, train=True, transform=trans, download=True)\n",
        "        self.val = torchvision.datasets.FashionMNIST(\n",
        "            root=self.root, train=False, transform=trans, download=True)\n",
        "\n",
        "@d2l.add_to_class(FashionMNIST)  #@save\n",
        "def text_labels(self, indices):\n",
        "    \"\"\"Return text labels.\"\"\"\n",
        "    labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "              'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "    return [labels[int(i)] for i in indices]\n",
        "\n",
        "@d2l.add_to_class(FashionMNIST)  #@save\n",
        "def get_dataloader(self, train):\n",
        "    data = self.train if train else self.val\n",
        "    return torch.utils.data.DataLoader(data, self.batch_size, shuffle=train,\n",
        "                                       num_workers=self.num_workers)\n",
        "\n",
        "@d2l.add_to_class(FashionMNIST)  #@save\n",
        "def visualize(self, batch, nrows=1, ncols=8, labels=[]):\n",
        "    X, y = batch\n",
        "    if not labels:\n",
        "        labels = self.text_labels(y)\n",
        "    d2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)\n",
        "\n",
        "data_batch_1 = FashionMNIST(batch_size=1, resize=(32, 32))\n",
        "tic = time.time()\n",
        "for X, y in data_batch_1.train_dataloader():\n",
        "    continue\n",
        "f'Time taken with batch_size=1: {time.time() - tic:.2f} sec'"
      ],
      "id": "26c54b96",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Time taken with batch_size=1: 147.79 sec'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bff1562",
      "metadata": {
        "origin_pos": 31,
        "tab": [
          "pytorch"
        ],
        "id": "7bff1562"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/49)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecae7560"
      },
      "source": [
        "# Task\n",
        "Analyze the performance of the data iterator, identify bottlenecks using a system profiler, explore and implement optimizations, and determine if the current implementation is fast enough."
      ],
      "id": "ecae7560"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "070bac16"
      },
      "source": [
        "## Evaluate current performance\n",
        "\n",
        "### Subtask:\n",
        "Re-run the data loading loop with the original batch size (64) to confirm the baseline performance.\n"
      ],
      "id": "070bac16"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72cf9c36"
      },
      "source": [
        "**Reasoning**:\n",
        "Re-running the data loading loop with the original batch size (64) is necessary to confirm the baseline performance before exploring optimizations, as requested in the subtask. This involves instantiating the data module with the specified batch size, starting a timer, iterating through the dataloader, stopping the timer, and printing the elapsed time.\n",
        "\n"
      ],
      "id": "72cf9c36"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "e5f9ed5f",
        "outputId": "fa59c98f-b89b-4c6d-e2b9-8cfdd84dcfc7"
      },
      "source": [
        "data_batch_64 = FashionMNIST(batch_size=64, resize=(32, 32))\n",
        "tic = time.time()\n",
        "for X, y in data_batch_64.train_dataloader():\n",
        "    continue\n",
        "f'Time taken with batch_size=64: {time.time() - tic:.2f} sec'"
      ],
      "id": "e5f9ed5f",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Time taken with batch_size=64: 11.95 sec'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6550ea41"
      },
      "source": [
        "## Identify bottlenecks\n",
        "\n",
        "### Subtask:\n",
        "Use a system profiler to analyze the data loading process and identify where the bottlenecks are occurring.\n"
      ],
      "id": "6550ea41"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "263677a5"
      },
      "source": [
        "**Reasoning**:\n",
        "Profile the data loading loop to identify performance bottlenecks.\n",
        "\n"
      ],
      "id": "263677a5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fecc24df",
        "outputId": "997f27ab-b674-48f5-c651-eb11f326c10d"
      },
      "source": [
        "import cProfile\n",
        "import pstats\n",
        "\n",
        "data_batch_64 = FashionMNIST(batch_size=64, resize=(32, 32))\n",
        "\n",
        "profiler = cProfile.Profile()\n",
        "profiler.enable()\n",
        "\n",
        "for X, y in data_batch_64.train_dataloader():\n",
        "    continue\n",
        "\n",
        "profiler.disable()\n",
        "\n",
        "stats = pstats.Stats(profiler)\n",
        "stats.sort_stats('cumulative')\n",
        "stats.print_stats(20)"
      ],
      "id": "fecc24df",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         630348 function calls (618871 primitive calls) in 18.372 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 502 to 20 due to restriction <20>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "      939    0.023    0.000   34.527    0.037 /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:729(__next__)\n",
            "      3/2    0.000    0.000   18.371    9.186 /usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3512(run_code)\n",
            "      3/2    0.000    0.000   18.370    9.185 {built-in method builtins.exec}\n",
            "        8    0.001    0.000   18.370    2.296 /usr/lib/python3.12/multiprocessing/util.py:208(__call__)\n",
            "      2/1    0.000    0.000   18.370   18.370 /tmp/ipython-input-3992456402.py:1(<cell line: 0>)\n",
            "3778/3767    0.020    0.000   18.282    0.005 {built-in method posix.close}\n",
            "1884/1880    0.006    0.000   18.280    0.010 /usr/lib/python3.12/multiprocessing/connection.py:174(close)\n",
            "1886/1882    0.003    0.000   18.274    0.010 /usr/lib/python3.12/multiprocessing/connection.py:376(_close)\n",
            "        4    0.000    0.000   18.262    4.565 /usr/lib/python3.12/multiprocessing/util.py:466(close_fds)\n",
            "      4/1    0.000    0.000   18.255   18.255 /usr/lib/python3.12/threading.py:1018(_bootstrap)\n",
            "      4/1    0.001    0.000   18.255   18.255 /usr/lib/python3.12/threading.py:1058(_bootstrap_inner)\n",
            "      4/1    0.000    0.000   18.255   18.255 /usr/lib/python3.12/threading.py:1001(run)\n",
            "      4/1    0.000    0.000   18.254   18.254 /usr/lib/python3.12/multiprocessing/queues.py:231(_feed)\n",
            "      939    0.022    0.000   17.298    0.018 /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:1458(_next_data)\n",
            "      938    0.004    0.000   16.973    0.018 /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:1423(_get_data)\n",
            "      938    0.005    0.000   16.970    0.018 /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:1272(_try_get_data)\n",
            "      938    0.012    0.000   16.814    0.018 /usr/lib/python3.12/multiprocessing/queues.py:98(get)\n",
            "      938    0.116    0.000   12.701    0.014 {built-in method _pickle.loads}\n",
            "     1876    0.038    0.000   12.406    0.007 /usr/local/lib/python3.12/dist-packages/torch/multiprocessing/reductions.py:540(rebuild_storage_fd)\n",
            "     1876    0.020    0.000   12.198    0.007 /usr/lib/python3.12/multiprocessing/resource_sharer.py:55(detach)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pstats.Stats at 0x7db806aede20>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eac699a5"
      },
      "source": [
        "## Explore optimization options\n",
        "\n",
        "### Subtask:\n",
        "Based on the profiling results, research and explore potential ways to improve the data loading performance. This could involve increasing the number of worker processes, using different data loading techniques, or optimizing data transformations.\n"
      ],
      "id": "eac699a5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99a8f3d1"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the profiling results that showed bottlenecks in pickling and multiprocessing, the next step is to research potential solutions to improve data loading performance, focusing on these areas.\n",
        "\n"
      ],
      "id": "99a8f3d1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f06125e6",
        "outputId": "28e9baab-8438-4a76-c818-5a82debb57ef"
      },
      "source": [
        "# This is a placeholder for research findings and discussion.\n",
        "# In a real scenario, this would involve external research.\n",
        "\n",
        "# Potential strategies to address pickling/multiprocessing bottlenecks:\n",
        "# 1. Increase num_workers: Allows for more parallel data loading. However,\n",
        "#    this can increase memory usage and overhead from multiprocessing.\n",
        "#    The warning about exceeding suggested workers suggests a potential\n",
        "#    system limitation or configuration issue.\n",
        "# 2. Use a faster serialization method: While standard pickling is the default,\n",
        "#    alternatives like 'dill' or 'torch.save'/'torch.load' might offer\n",
        "#    better performance, though integrating them into the DataLoader\n",
        "#    might require custom dataset implementations.\n",
        "# 3. Optimize data transformations: Although not the primary bottleneck\n",
        "#    identified, ensuring transformations are efficient and potentially\n",
        "#    applied within the worker processes can help.\n",
        "# 4. Pin memory: For GPU training, pinning memory can speed up data transfer\n",
        "#    to the GPU. This doesn't directly address the CPU-side bottleneck\n",
        "#    but can improve overall training time.\n",
        "# 5. Explore alternative data loading libraries: Libraries like DALI (NVIDIA)\n",
        "#    are specifically designed for high-performance data loading and augmentation,\n",
        "#    especially for deep learning.\n",
        "\n",
        "# Investigating the effect of increasing num_workers:\n",
        "# We can experiment with different values for num_workers to see the impact\n",
        "# on loading time, keeping in mind the system's resources.\n",
        "\n",
        "# Exploring alternative data loading approaches:\n",
        "# Custom Datasets: If the standard DataLoader is insufficient, a custom Dataset\n",
        "# class could be implemented to handle data loading and preprocessing\n",
        "# in a more optimized way, potentially using different multiprocessing\n",
        "# or threading models.\n",
        "\n",
        "# Optimizing data transformations:\n",
        "# The current transformations are simple (Resize, ToTensor). More complex\n",
        "# transformations might require optimization, but for this dataset, they\n",
        "# are likely not the main issue based on profiling.\n",
        "\n",
        "print(\"Research on data loading optimizations completed.\")"
      ],
      "id": "f06125e6",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Research on data loading optimizations completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15d568c0"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the research, increasing `num_workers` is a common strategy to improve data loading performance by leveraging multiprocessing. The next logical step is to experiment with different values for `num_workers` to see if it alleviates the bottleneck identified in the profiling step.\n",
        "\n"
      ],
      "id": "15d568c0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbfa90ed",
        "outputId": "5c6cca57-b85c-40c5-a7e8-97c3ef2a9213"
      },
      "source": [
        "# Experimenting with different num_workers values\n",
        "num_workers_to_test = [0, 1, 2, 4, 8] # Include 0 for single-process baseline\n",
        "\n",
        "results = {}\n",
        "\n",
        "for num_workers in num_workers_to_test:\n",
        "    print(f\"Testing with num_workers = {num_workers}\")\n",
        "    data_test = FashionMNIST(batch_size=64, resize=(32, 32))\n",
        "    # Temporarily override num_workers in the dataloader getter for testing\n",
        "    original_get_dataloader = data_test.get_dataloader\n",
        "    def custom_get_dataloader(train):\n",
        "        data = data_test.train if train else data_test.val\n",
        "        return torch.utils.data.DataLoader(data, data_test.batch_size, shuffle=train,\n",
        "                                           num_workers=num_workers)\n",
        "    data_test.get_dataloader = custom_get_dataloader\n",
        "\n",
        "    tic = time.time()\n",
        "    for X, y in data_test.train_dataloader():\n",
        "        continue\n",
        "    time_taken = time.time() - tic\n",
        "    results[num_workers] = time_taken\n",
        "    print(f\"Time taken with num_workers = {num_workers}: {time_taken:.2f} sec\")\n",
        "\n",
        "print(\"\\nPerformance results with different num_workers:\")\n",
        "for num_workers, time_taken in results.items():\n",
        "    print(f\"num_workers = {num_workers}: {time_taken:.2f} sec\")\n",
        "\n",
        "# Restore the original get_dataloader method if necessary for subsequent steps\n",
        "# Although in this case, the object 'data_test' will go out of scope."
      ],
      "id": "fbfa90ed",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with num_workers = 0\n",
            "Time taken with num_workers = 0: 10.73 sec\n",
            "Testing with num_workers = 1\n",
            "Time taken with num_workers = 1: 14.37 sec\n",
            "Testing with num_workers = 2\n",
            "Time taken with num_workers = 2: 11.65 sec\n",
            "Testing with num_workers = 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken with num_workers = 4: 11.73 sec\n",
            "Testing with num_workers = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken with num_workers = 8: 14.03 sec\n",
            "\n",
            "Performance results with different num_workers:\n",
            "num_workers = 0: 10.73 sec\n",
            "num_workers = 1: 14.37 sec\n",
            "num_workers = 2: 11.65 sec\n",
            "num_workers = 4: 11.73 sec\n",
            "num_workers = 8: 14.03 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0c0cfe3"
      },
      "source": [
        "## Implement and test optimizations\n",
        "\n",
        "### Subtask:\n",
        "Implement and test optimizations to improve data loading performance.\n"
      ],
      "id": "a0c0cfe3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14b07c48"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the optimization by setting `num_workers` to 0 in the `FashionMNIST` class and measure the time taken to load the training data.\n",
        "\n"
      ],
      "id": "14b07c48"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "ff5f7744",
        "outputId": "43892183-5236-494b-ec6b-039a40dc2ad5"
      },
      "source": [
        "class FashionMNIST(d2l.DataModule):  #@save\n",
        "    \"\"\"The Fashion-MNIST dataset.\"\"\"\n",
        "    def __init__(self, batch_size=64, resize=(28, 28), num_workers=0):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        trans = transforms.Compose([transforms.Resize(resize),\n",
        "                                    transforms.ToTensor()])\n",
        "        self.train = torchvision.datasets.FashionMNIST(\n",
        "            root=self.root, train=True, transform=trans, download=True)\n",
        "        self.val = torchvision.datasets.FashionMNIST(\n",
        "            root=self.root, train=False, transform=trans, download=True)\n",
        "\n",
        "    @d2l.add_to_class(FashionMNIST)\n",
        "    def text_labels(self, indices):\n",
        "        \"\"\"Return text labels.\"\"\"\n",
        "        labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "                  'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "        return [labels[int(i)] for i in indices]\n",
        "\n",
        "    @d2l.add_to_class(FashionMNIST)\n",
        "    def get_dataloader(self, train):\n",
        "        data = self.train if train else self.val\n",
        "        return torch.utils.data.DataLoader(data, self.batch_size, shuffle=train,\n",
        "                                           num_workers=self.num_workers)\n",
        "\n",
        "    @d2l.add_to_class(FashionMNIST)\n",
        "    def visualize(self, batch, nrows=1, ncols=8, labels=[]):\n",
        "        X, y = batch\n",
        "        if not labels:\n",
        "            labels = self.text_labels(y)\n",
        "        d2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)\n",
        "\n",
        "data_optimized = FashionMNIST(batch_size=64, resize=(32, 32), num_workers=0)\n",
        "tic = time.time()\n",
        "for X, y in data_optimized.train_dataloader():\n",
        "    continue\n",
        "time_taken_optimized = time.time() - tic\n",
        "print(f'Time taken with batch_size=64 and num_workers=0: {time_taken_optimized:.2f} sec')"
      ],
      "id": "ff5f7744",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3908238613.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdata_optimized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFashionMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_optimized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mtime_taken_optimized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d2l/torch.py\u001b[0m in \u001b[0;36mtrain_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3009c8c"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `TypeError: 'NoneType' object is not callable` indicates that `self.get_dataloader` is None. This is likely because adding the `get_dataloader` method to the class using `d2l.add_to_class` after the class definition might not be working as expected in this interactive environment, or the method is not being correctly assigned. I will include the `get_dataloader` method directly within the class definition to ensure it is properly associated with the `FashionMNIST` class.\n",
        "\n"
      ],
      "id": "b3009c8c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzroXtBFYcZm",
        "outputId": "a992b9df-cfd7-41b9-df55-788ba0fc7751"
      },
      "source": [
        "class FashionMNIST(d2l.DataModule):\n",
        "    \"\"\"The Fashion-MNIST dataset.\"\"\"\n",
        "    def __init__(self, batch_size=64, resize=(28, 28), num_workers=0):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        trans = transforms.Compose([transforms.Resize(resize),\n",
        "                                    transforms.ToTensor()])\n",
        "        self.train = torchvision.datasets.FashionMNIST(\n",
        "            root=self.root, train=True, transform=trans, download=True)\n",
        "        self.val = torchvision.datasets.FashionMNIST(\n",
        "            root=self.root, train=False, transform=trans, download=True)\n",
        "\n",
        "    def text_labels(self, indices):\n",
        "        \"\"\"Return text labels.\"\"\"\n",
        "        labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "                  'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "        return [labels[int(i)] for i in indices]\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        data = self.train if train else self.val\n",
        "        return torch.utils.data.DataLoader(data, self.batch_size, shuffle=train,\n",
        "                                           num_workers=self.num_workers)\n",
        "\n",
        "    def visualize(self, batch, nrows=1, ncols=8, labels=[]):\n",
        "        X, y = batch\n",
        "        if not labels:\n",
        "            labels = self.text_labels(y)\n",
        "        d2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)\n",
        "\n",
        "data_optimized = FashionMNIST(batch_size=64, resize=(32, 32), num_workers=0)\n",
        "tic = time.time()\n",
        "for X, y in data_optimized.train_dataloader():\n",
        "    continue\n",
        "time_taken_optimized = time.time() - tic\n",
        "print(f'Time taken with batch_size=64 and num_workers=0: {time_taken_optimized:.2f} sec')\n",
        "\n",
        "# Compare with the baseline time (assuming the baseline was around 11.95 seconds from previous runs)\n",
        "# You would typically have this baseline stored in a variable. For this example,\n",
        "# I'll use a placeholder value based on the previous execution.\n",
        "baseline_time = 11.95 # Placeholder based on previous execution result\n",
        "\n",
        "print(f'Baseline time with default num_workers: {baseline_time:.2f} sec')\n",
        "print(f'Improvement (lower is better): {time_taken_optimized - baseline_time:.2f} sec')"
      ],
      "id": "zzroXtBFYcZm",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken with batch_size=64 and num_workers=0: 10.74 sec\n",
            "Baseline time with default num_workers: 11.95 sec\n",
            "Improvement (lower is better): -1.21 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ff74f63"
      },
      "source": [
        "## Analyze and conclude\n",
        "\n",
        "### Subtask:\n",
        "Based on the results, determine if the current implementation is fast enough for the intended use case and summarize the findings on how the performance was affected by the optimizations.\n"
      ],
      "id": "3ff74f63"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a419382"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the results of the optimized data loading and compare it to the baseline to determine if it's fast enough and summarize the findings as requested.\n",
        "\n"
      ],
      "id": "5a419382"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bd188da",
        "outputId": "9be8cfd2-56d6-47ee-8220-bd240ddd2b59"
      },
      "source": [
        "# Review the results\n",
        "time_taken_optimized = 10.736563920974731 # from the previous execution\n",
        "baseline_time = 11.95 # from previous executions\n",
        "\n",
        "print(f\"Optimized data loading time (num_workers=0): {time_taken_optimized:.2f} sec\")\n",
        "print(f\"Baseline data loading time (default num_workers): {baseline_time:.2f} sec\")\n",
        "\n",
        "# Determine if fast enough\n",
        "# Typical training times for Fashion-MNIST are much longer than ~11 seconds per epoch.\n",
        "# A single epoch of training a simple CNN on Fashion-MNIST can take several minutes to tens of minutes\n",
        "# on a standard CPU or GPU. If data loading takes ~11 seconds per epoch, and training takes minutes,\n",
        "# the data loading is unlikely to be the primary bottleneck.\n",
        "\n",
        "if time_taken_optimized < baseline_time:\n",
        "    print(\"\\nOptimization (num_workers=0) improved data loading performance.\")\n",
        "    print(f\"Improvement: {baseline_time - time_taken_optimized:.2f} sec\")\n",
        "else:\n",
        "     print(\"\\nOptimization (num_workers=0) did not improve data loading performance compared to baseline.\")\n",
        "\n",
        "print(\"\\nSummary of findings:\")\n",
        "print(f\"- Baseline data loading time with default num_workers was approximately {baseline_time:.2f} seconds.\")\n",
        "print(f\"- Setting num_workers to 0 resulted in an optimized data loading time of approximately {time_taken_optimized:.2f} seconds.\")\n",
        "print(f\"- This represents an improvement of approximately {baseline_time - time_taken_optimized:.2f} seconds.\")\n",
        "print(\"- The profiling results indicated that multiprocessing and pickling overhead were bottlenecks with default num_workers.\")\n",
        "print(\"- Disabling multiprocessing by setting num_workers=0 appears to have reduced this overhead in this specific environment.\")\n",
        "print(\"- Compared to typical deep learning model training times on Fashion-MNIST (minutes per epoch), the optimized data loading time of ~11 seconds is unlikely to be a significant bottleneck.\")\n",
        "print(\"- Therefore, the current optimized implementation is considered fast enough for the intended use case.\")\n"
      ],
      "id": "3bd188da",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized data loading time (num_workers=0): 10.74 sec\n",
            "Baseline data loading time (default num_workers): 11.95 sec\n",
            "\n",
            "Optimization (num_workers=0) improved data loading performance.\n",
            "Improvement: 1.21 sec\n",
            "\n",
            "Summary of findings:\n",
            "- Baseline data loading time with default num_workers was approximately 11.95 seconds.\n",
            "- Setting num_workers to 0 resulted in an optimized data loading time of approximately 10.74 seconds.\n",
            "- This represents an improvement of approximately 1.21 seconds.\n",
            "- The profiling results indicated that multiprocessing and pickling overhead were bottlenecks with default num_workers.\n",
            "- Disabling multiprocessing by setting num_workers=0 appears to have reduced this overhead in this specific environment.\n",
            "- Compared to typical deep learning model training times on Fashion-MNIST (minutes per epoch), the optimized data loading time of ~11 seconds is unlikely to be a significant bottleneck.\n",
            "- Therefore, the current optimized implementation is considered fast enough for the intended use case.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "112dd712"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The baseline data loading time with the default number of workers and a batch size of 64 was approximately 11.95 seconds.\n",
        "*   Profiling revealed that a significant portion of the data loading time was spent in `_pickle.loads` and functions related to `multiprocessing`, indicating bottlenecks in data transfer and unpickling between worker processes and the main process.\n",
        "*   Experimentation showed that increasing `num_workers` did not improve performance and, in fact, degraded it, with `num_workers=0` (single-process loading) being the fastest configuration tested.\n",
        "*   Setting `num_workers` to 0 in the `FashionMNIST` dataloader reduced the data loading time to approximately 10.74 seconds, an improvement of about 1.21 seconds compared to the baseline.\n",
        "*   This optimization by disabling multiprocessing likely reduced the overhead associated with inter-process communication and pickling in this specific environment.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The optimized data loading time of approximately 10.74 seconds per epoch is unlikely to be a bottleneck compared to typical deep learning model training times on Fashion-MNIST, suggesting the current implementation is fast enough.\n",
        "*   Further performance improvements, if needed, would likely require exploring alternative data loading methods or libraries (e.g., DALI) or optimizing data transformations outside the standard `DataLoader` framework, as simply adjusting `num_workers` was not effective.\n"
      ],
      "id": "112dd712"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}